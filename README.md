# HunyuanOCR Inference & Fine-tuning Project

æœ¬é¡¹ç›®åŸºäºè…¾è®¯æ··å…ƒå›¢é˜Ÿå¼€æºçš„ [HunyuanOCR](https://github.com/Tencent-Hunyuan/HunyuanOCR) æ„å»ºï¼Œæ”¯æŒæ··å…ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–‡æœ¬æ£€æµ‹ä¸è¯†åˆ«ä»»åŠ¡æ¨ç†ï¼Œä»¥åŠåç»­åŸºäº verl çš„åè®­ç»ƒã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

HunyuanOCR æ˜¯è…¾è®¯æ¨å‡ºçš„å¤šæ¨¡æ€ OCR æ¨¡å‹ï¼Œå…·å¤‡å¼ºå¤§çš„å›¾æ–‡ç†è§£ä¸æ–‡æœ¬è¯†åˆ«èƒ½åŠ›ï¼Œæ”¯æŒå¤æ‚åœºæ™¯ä¸‹çš„æ–‡å­—æ£€æµ‹ã€è¯†åˆ«åŠç»“æ„åŒ–è¾“å‡ºã€‚æœ¬é¡¹ç›®æä¾›SFTè®­ç»ƒï¼Œåç»­æä¾›åŸºäºverlçš„åè®­ç»ƒä»£ç 

## ğŸ› ï¸ ç¯å¢ƒå®‰è£…

### ç³»ç»Ÿè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Linux
- **Python**: 3.12+ (æ¨èå¹¶æµ‹è¯•ç‰ˆæœ¬)
- **CUDA**: 12.9
- **PyTorch**: 2.7.1
- **GPU**: æ”¯æŒ CUDA çš„ NVIDIA GPU
- **æ˜¾å­˜**: â‰¥20GB (ç”¨äº vLLM)
- **ç£ç›˜ç©ºé—´**: â‰¥6GB

### å®‰è£…æ­¥éª¤


1. **å…‹éš†ä»“åº“**
   ```bash
   git clone https://github.com/luxiaolili/HunyuanOCR_Train.git
   cd HunyuanOCR_Train
   ```
2. **ä¿®æ”¹å®˜æ–¹çš„HunYuanVLForConditionalGeneration ä»£ç **
   ### å®˜æ–¹çš„ä»£ç forwardä¸­æ²¡æœ‰ä¼ å…¥vitå›¾ç‰‡çš„ç‰¹å¾ï¼Œéœ€è¦ä¿®æ”¹
   ```
   class HunYuanVLForConditionalGeneration(HunYuanVLPreTrainedModel, GenerationMixin):
      _tied_weights_keys = ["lm_head.weight"]
      config: HunYuanVLConfig
      
      def __init__(self, config: HunYuanVLConfig):
          super().__init__(config)
          self.model = HunYuanVLModel(config)
          self.vocab_size = config.vocab_size
          self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
          self.vit = HunYuanVisionTransformer(config.vision_config)
          self.config = config
          self.post_init()
      
      def set_decoder(self, decoder):
          self.model = decoder
      
      def get_decoder(self):
          return self.model
      
      @can_return_tuple
      @auto_docstring
      def forward(
          self,
          input_ids: Optional[torch.LongTensor] = None,
          attention_mask: Optional[torch.Tensor] = None,
          position_ids: Optional[torch.LongTensor] = None,
          past_key_values: Optional[Cache] = None,
          pixel_values: Optional[torch.FloatTensor] = None,
          image_grid_thw: Optional[torch.FloatTensor] = None,
          inputs_embeds: Optional[torch.FloatTensor] = None,
          labels: Optional[torch.LongTensor] = None,
          use_cache: Optional[bool] = None,
          cache_position: Optional[torch.LongTensor] = None,
          logits_to_keep: Union[int, torch.Tensor] = 0,
          **kwargs: Unpack[TransformersKwargs],
      ) -> CausalLMOutputWithPast:
          r"""
          Example:
      
          ```python
          >>> from transformers import AutoProcessor, HunYuanVLForConditionalGeneration
          >>> from PIL import Image
          >>> import torch
      
          >>> model_name_or_path = "tencent/HunyuanOCR"
          >>> processor = AutoProcessor.from_pretrained(model_name_or_path, use_fast=False)
          >>> model = HunYuanVLForConditionalGeneration.from_pretrained(
          ...     model_name_or_path,
          ...     attn_implementation="eager",
          ...     torch_dtype=torch.bfloat16,
          ...     device_map="auto",
          ... )
      
          >>> img_path = "path/to/your/image.jpg"
          >>> image = Image.open(img_path).convert("RGB")
      
          >>> messages = [
          ...     {
          ...         "role": "user",
          ...         "content": [
          ...             {"type": "image", "image": img_path},
          ...             {"type": "text", "text": "Extract the text from the image."},
          ...         ],
          ...     }
          ... ]
          >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
          >>> inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt").to(model.device)
      
          >>> with torch.no_grad():
          ...     generated_ids = model.generate(**inputs, max_new_tokens=1024)
          >>> generated_ids_trimmed = generated_ids[0][len(inputs["input_ids"][0]):]
          >>> output = processor.decode(generated_ids_trimmed, skip_special_tokens=True)
      
          >>> print(output)
      
          ```"""
      
          
          if inputs_embeds is None:
              inputs_embeds = self.model.embed_tokens(input_ids).clone()
             
          if  pixel_values is not None:
              pixel_values = pixel_values.to(torch.bfloat16)
              image_embeds = self.vit(pixel_values, image_grid_thw)
      
              # ViT may be deployed on different GPUs from those used by LLMs, due to auto-mapping of accelerate.
              image_embeds = image_embeds.to(input_ids.device, non_blocking=True)
      
              image_mask, _ = self.get_placeholder_mask(
                  input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds
              )
              inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
          
          outputs: BaseModelOutputWithPast = self.model(
              input_ids=None,
              attention_mask=attention_mask,
              position_ids=position_ids,
              past_key_values=past_key_values,
              inputs_embeds=inputs_embeds,
              use_cache=use_cache,
              cache_position=cache_position,
              **kwargs,
          )
      
          hidden_states = outputs.last_hidden_state
          # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
          slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
          logits = self.lm_head(hidden_states[:, slice_indices, :])
      
          loss = None
          if labels is not None:
              loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
      
          return CausalLMOutputWithPast(
              loss=loss,
              logits=logits,
              past_key_values=outputs.past_key_values,
              hidden_states=outputs.hidden_states,
              attentions=outputs.attentions,
          )
   ```

2. **æ•°æ®é›†**
   ###
   train.jsonl, test.jsonl. æ··å…ƒçš„special tokenå’Œå…¶ä»–å¼€æºçš„vlmçš„ä¸åŒã€‚<hy_place_holder_no_112> text <hy_place_holder_no_113> <hy_place_holder_no_110>(x1, y1)(x2, y2) <hy_place_holder_no_110> templateå’Œå…¶ä»–çš„ä¹Ÿä¸
   ç›¸åŒã€‚ å…¶ä»–é‡‡ç”¨<im_start>user xxx <im_start> assistant xxx.è…¾è®¯vlmçš„æ˜¯ xxx <| hy_User |> xxx <| hy_Assistant|
   ```
   æ ¼å¼ï¼š
   {"image": "xxx.png", "prompt":"æå–å›¾ä¸­çš„æ–‡å­—", "answer":"è®­ç»ƒOCRæ•°æ®"}
   ```
4. **è¿è¡Œ**
   
   ```
   run.sh
   ```

5. **é—®é¢˜**
   1. HunYuanOCRå¯¹Systemçš„promptæ•æ„Ÿ
   2. NERä»»åŠ¡éœ€è¦SFTæå‡
   
6. **todo**
   - [x] SFTè®­ç»ƒ
   - [ ] Verlåè®­ç»ƒ


